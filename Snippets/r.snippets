snippet lib
	library(${1:package})

snippet req
	require(${1:package})

snippet src
	source("${1:file.R}")

snippet ret
	return(${1:code})

snippet mat
	matrix(${1:data}, nrow = ${2:rows}, ncol = ${3:cols})

snippet sg
	setGeneric("${1:generic}", function(${2:x, ...}) {
		standardGeneric("${1:generic}")
	})

snippet sm
	setMethod("${1:generic}", ${2:class}, function(${2:x, ...}) {
		${0}
	})

snippet sc
	setClass("${1:Class}", slots = c(${2:name = "type"}))

snippet if
	if (${1:condition}) {
		${0}
	}

snippet el
	else {
		${0}
	}

snippet ei
	else if (${1:condition}) {
		${0}
	}

snippet fun
	${1:name} <- function(${2:variables}) {
		${0}
	}

snippet for
	for (${1:variable} in ${2:vector}) {
		${0}
	}

snippet while
	while (${1:condition}) {
		${0}
	}

snippet switch
	switch (${1:object},
		${2:case} = ${3:action}
	)

snippet apply
	apply(${1:array}, ${2:margin}, ${3:...})

snippet lapply
	lapply(${1:list}, ${2:function})

snippet sapply
	sapply(${1:list}, ${2:function})

snippet mapply
	mapply(${1:function}, ${2:...})

snippet tapply
	tapply(${1:vector}, ${2:index}, ${3:function})

snippet vapply
	vapply(${1:list}, ${2:function}, FUN.VALUE = ${3:type}, ${4:...})

snippet rapply
	rapply(${1:list}, ${2:function})

snippet ts
	`r paste("#", date(), "------------------------------\n")`

snippet shinyapp
	library(shiny)
	
	ui <- fluidPage(
	  ${0}
	)
	
	server <- function(input, output, session) {
	  
	}
	
	shinyApp(ui, server)

snippet shinymod
	${1:name}_UI <- function(id) {
	  ns <- NS(id)
	  tagList(
		${0}
	  )
	}
	
	${1:name} <- function(input, output, session) {
	  
	}

################################
#Flowchart Data Resource Snippets
################################

snippet fdr_alpha
	#Determines alpha value based on sample size and BIC criterion
	alpha = 1-pchisq(log(nrow(${1:dataset})),1)
	
snippet fdr_linreg
	#Creates a linear regression model that is a function of all predictor variables
	lm.model = lm(${1:yvar} ~ . , data = ${2:dataset})

snippet fdr_globalF
	#Global F test: Determines if at least one variable is significant
	lm.model <- lm(${1:targetvar} ~ ${2:predvar1} + ${3:predvar2}, data = ${4:dataset})
	summary(lm.model)

snippet fdr_assumptions_linearity
	#Check to see if the below plot is random with an average value of zero
	library(ggplot2)
	lm.model=lm(${1:targetvar}~.,data=${2:dataset})
	ggplot(lm.model,aes(x=fitted(lm.model),y=resid(lm.model))) +
		geom_point(color="blue")+
		labs(x="Predicted Values",y="Residuals")



snippet fdr_assumptions_constvar 
	#Check to see if the below plot has a constant spread over the range of predicted values
	library(ggplot2)
	lm.model=lm(${1:targetvar}~.,data=${2:dataset})
	ggplot(lm.model,aes(x=fitted(lm.model),y=resid(lm.model))) +
		geom_point(color="blue") +
		labs(x="Predicted Values",y="Residuals")


snippet fdr_assumptions_normdist
	#Can use QQ-plot, Anderson-Darling test, or Shapiro-Wilk test
	library(ggplot2)
	library(nortest)
	lm.model=lm(${1:targetvar}~.,data=${2:dataset})
	
	ggplot(data = ${2:dataset}, aes(sample = lm.model\$residuals)) + #i. QQ-Plot
		stat_qq() +
		stat_qq_line()
		
	ad.test(lm.model\$residuals) #ii. Anderson-Darling test. Gives more weight to tails
	shapiro.test(lm.model\$residuals) #iii. Shapiro-Wilk test. Better for smaller data sets


snippet fdr_assumptions_independence
	#Can use plot to visually see time-dependence or use Durbin-Watson test
	library(ggplot2)
	library(lmtest)
	lm.model=lm(${1:targetvar}~.,data=${2:dataset})

	ggplot(train,aes(x=${3:timevariable},y=lm.model\$residuals)) +  #i. Visual Plot
		geom_point(color="blue") +
		labs( x="Time",y="Residuals")

	lm.model=lm(${1:targetvar}~${3:timevariable},data=${2:dataset})
	dwtest(lm.model,alternative="greater") #ii. Durbin-Watson test


snippet fdr_assumptions_multicol
	#Check for correlation among predictor variables or calculate the variance inflation factor (VIF)
	library(car)
	
	#First filter dataframe to only contain numeric variables
	${1:dataset}_num = ${1:dataset}[,c(5,6,82)]
	cor(${1:dataset}_num) #i. Correlation among predictor variables
	
	lm.model=lm(${2:targetvar}~.,data=${1:dataset})
	vif(lm.model) #ii. Variance inflation factor (VIF)


snippet fdr_outliers
	#Tests for finding outliers
	library(ggplot2)
	library(pdp)
	lm.model=lm(${1:targetvar}~.,data=${2:dataset})
	n.index = seq(1, nrow(${2:dataset}))
	
	#i. Internal Studentized Residuals
	a = ggplot(lm.model,aes(x=n.index,y=rstandard(lm.model))) + 
		geom_point(color="orange") + 
		geom_line(y=-3) + 
		geom_line(y=3) + 
		labs(title = "Internal Studentized Residuals",x="Observation",y="Residuals")
	
	#ii. External Studentized Residuals
	b = ggplot(lm.model,aes(x=n.index,y=rstudent(lm.model)))+ 
		geom_point(color="orange") + 
		geom_line(y=-3) + 
		geom_line(y=3) + 
		labs(title = "External Studentized Residuals",x="Observation",y="Residuals")
	
	grid.arrange(a,b)

	
snippet fdr_influential
	#Tests for finding influential observations
	library(ggplot2)
	library(pdp)
	lm.model=lm(${1:targetvar}~.,data=${2:dataset})
	n.index = seq(1, nrow(${2:dataset}))
	p = length(lm.model\$coefficients)
	
	#i. Cook's D
	D.cut= 4/(nrow(${2:dataset})-p-1)
	
	c = ggplot(lm.model,aes(x=n.index,y=cooks.distance(lm.model))) +
				geom_point(color="orange") +
				geom_line(y=D.cut) +
				labs(title = "Cook's D",x = "Observation",y ="Cook's Distance")

	#ii. DFFITS
	df.cut=2*(sqrt((p)/nrow(${2:dataset})))
	
	d = ggplot(lm.model,aes(x=n.index,y=dffits(lm.model))) +
				geom_point(color="orange") +
				geom_line(y=df.cut) +
				geom_line(y=-df.cut) +
				labs(title = "DFFITS",x="Observation",y="DFFITS")

	#Observations that are outside of dffits
	${2:dataset}[dffits(lm.model) > df.cut | dffits(lm.model) < -df.cut,]
	
	#iii. DFBETAS (copy and paste for each variable)
	db.cut=2/sqrt(nrow(${2:dataset}))

	e = ggplot(lm.model,aes(x=n.index,y=dfbetas(lm.model)[,'${3:variablename}'])) + 
				geom_point(color="orange") + 
				geom_line(y=db.cut) + 
				geom_line(y=-db.cut) + 
				labs(title = "DFBETA for ${3:variablename}",x="Observation",y="DFBETAS")
	
	#iv. Hhat
	hat.cut=2*(p+1)/nrow(${2:dataset})
	
	f = ggplot(lm.model,aes(x=n.index,y=hatvalues(lm.model))) +
				geom_point(color="orange") +
				geom_line(y=hat.cut) +
				labs(title = "Hat values",x="Observation",y="Hat Values")

	#Print all plots in a grid. Add additional variables for DFBETAS if desired
	grid.arrange(c,d,e,f)

snippet fdr_varselect_linreg
	#Create training model
	library(dplyr)
	library(glmnet)
	set.seed(19054)
	
	${1:dataframe} <- ${1:dataframe} %>% mutate(id = row_number()) #Only necessary if data does not have id-like column
	train = ${1:dataframe} %>% sample_frac(0.7) #70% of data points are set aside for training (can be changed)
	test <- anti_join(${1:dataframe}, train, by = 'id') #The remainder are assigned for testing
	train = train[,c(${2:colnumstovalidate})] #Select columns you want to analyze
	#train = train[,!(colnames(train) %in% "id")] #Remove id column if not used in modeling
	
	# Create full model and empty model
	full.model = lm(${3:yvar} ~ . , data = train)
	empty.model = lm(${3:yvar} ~ 1, data = train)
	
	# Information Criteria (InfCrit)
	#AIC: k = 2
	#BIC: k = log(nrow(train))
	#p-value: k = qchisq(alpha.f , 1, lower.tail = FALSE)
	
	#Create model
	#alpha.f=0.20 #Only needed for p-value calculation
	
	for.model = step(empty.model, scope = list(lower = empty.model, upper = full.model), direction = "forward", k = ${5:InfCrit})
	back.model = step(full.model, scope = list(lower = empty.model, upper = full.model), direction = "backward", k = ${5:InfCrit})
	step.model = step(empty.model, scope = list(lower = empty.model, upper = full.model), direction = "both", k = ${5:InfCrit})
	
	#Regularized regression for variable selection (LASSO example)
	train_x <- model.matrix(${3:yvar} ~ ., data = train)[, -1]
	train_y <- train\$'${3:yvar}'
	
	train_lasso = glmnet(x=train_x, y = ${3:yvar}, alpha = 1)
	plot(train_lasso, xvar = "lambda")

snippet fdr_modeval
	#Evaluate various metrics for model accuracy
	library(dplyr)
	
	${1:testdataset}\$pred_lm <- predict(${2:finalmodel}, newdata = ${1:testdataset})
	
	${1:testdataset} %>% 
	  mutate(lm_APE = 100*abs((${3:yvar} - pred_lm)/${3:yvar})) %>% 
	  mutate(lm_AE = abs(${3:yvar} - pred_lm)) %>% 
	  mutate(lm_SE = (${3:yvar} - pred_lm)^2) %>% 
	  dplyr::summarise(MAPE_lm = mean(lm_APE)
															, MAE_lm = mean(lm_AE)
															, MSE_lm = mean(lm_SE)
															, RMSE_lm = sqrt(mean(lm_SE))
															, AIC_lm = AIC(${2:finalmodel})
															, BIC_lm = BIC(${2:finalmodel})
															, Ra2_lm = summary(${2:finalmodel})\$adj.r.squared)

##################################################################################
#Data Mining Snippets
##################################################################################		
#Snippet 1: fdr_dm_ktune (tuning k using caret)
snippet fdr_dm_ktune
	library(caret)
	library(ggplot2)
	set.seed(12345)
	grid = expand.grid(k = seq(1:20)) #Can adjust the range of k if needed
	
	tuned_knn = caret::train(factor(${1:ResponseVariable}) ~., method= "knn",
													data = subset(${2:train}, select = -c(${3:Column1ToExclude}, ${4:Column2ToExclude})),
													trControl = trainControl(method = 'cv',
																										number = 3,
																										search = "grid"),
													tuneGrid = grid)
	tuned_knn${5:$}results
	
	#Plot results to determine which k maximizes accuracy
	ggplot(tuned_knn${5:$}results, aes(x=k, y=Accuracy)) +
		geom_line()
	
	#The following line of code works as well
	tuned_knn${5:$}results[which.max(tuned_knn${5:$}results${5:$}Accuracy),]

#Snippet 2: fdr_dm_knn (run knn)
snippet fdr_dm_knn
	#Running KNN requires specifying:
	## (1) train.x: Training predictor variable(s) values
	## (2) test.x: Test predictor variable(s) values
	## (3) train.y: Training response variable values
	## (4) k: Number of clusters
	library(class)
	predict.test=class::knn(${1:train.x},${2:test.x},${3:train.y},k=${4:NumberOfClusters})
	
	#Determine misclassification rate
	sum(predict.test != ${3:train.y})/length(${3:train.y})

#Snippet 3: fdr_dm_classtree (create classification tree and evaluate variable importance)
snippet fdr_dm_classtree
	#Creates a classification tree
	library(rpart)
	library(rpart.plot)
	library(ggplot2)
	
	BC.tree = rpart(${1:ResponseVariable} ~ . , data = ${2:train}, method = 'class',
									parms = list(split='gini')) ## or 'information'
	summary(BC.tree)
	
	#Print the actual tree
	print(BC.tree)
	rpart.plot(BC.tree)
	
	#Variable importance
	BC.tree${3:$}variable.importance
	
	varimp.data = data.frame(BC.tree${3:$}variable.importance)
	varimp.data${3:$}names = as.character(rownames(varimp.data))
	
	ggplot(data = varimp.data,aes(x = fct_reorder(names,BC.tree${3:$}variable.importance), y = BC.tree${3:$}variable.importance)) +
		geom_bar(stat="identity") +
		coord_flip() +
		labs(x="Variable Name",y="Variable Importance")

#Snippet 4: fdr_dm_regtree (create regression tree and evaluate variable importance)
snippet fdr_dm_regtree
	library(rpart)
	library(rpart.plot)
	library(ggplot2)
	
	R.tree = rpart(${1:ResponseVariable} ~ . , data = ${2:train}
								, method = 'anova' #Regression tree
								, control = rpart.control(minsplit = 10)) #the minimum number of observations that must exist in a node in order for a split to be attempted.
	summary(R.tree)
	printcp(R.tree)

	#Print the actual tree
	print(R.tree)
	rpart.plot(R.tree)
	
	#Variable importance
	R.tree${3:$}variable.importance
	
	varimp.data = data.frame(R.tree${3:$}variable.importance)
	varimp.data${3:$}names = as.character(rownames(varimp.data))
	
	ggplot(data = varimp.data,aes(x = fct_reorder(names,R.tree${3:$}variable.importance), y = R.tree${3:$}variable.importance)) +
		geom_bar(stat="identity") +
		coord_flip() +
	labs(x="Variable Name",y="Variable Importance")
	
#Snippet 5: fdr_dm_evaluation (calculate evaluation metrics)
snippet fdr_dm_evaluation
	#Classification Evaluation metrics
	#Use CART model to predict new values for train and test datasets
	trainscores = predict(${1:ClassificationTreeVariable}, type='class') #Class for classification trees; vector for regression trees
	testscores = predict(${1:ClassificationTreeVariable}, test, type='class')
	
	#Training/testing misclassification rate:
	sum(trainscores!=${2:train}${6:$}${3:ResponseVariable})/nrow(${2:train})
	sum(testscores!=${4:test}${6:$}${3:ResponseVariable})/nrow(${4:test})

	#Regression Evaluation metrics
	#Use CART regression model to predict new values for train and test datasets
	trainscores = predict(${5:RegressionTreeVariable}, type='vector') #Class for classification trees; vector for regression trees
	testscores = predict(${5:RegressionTreeVariable}, test, type='vector')
	
	#Training/testing MAE and MAPE (regression):
	mean(abs(trainscores-train${6:$}${3:ResponseVariable}))
	mean(abs((trainscores-train${6:$}${3:ResponseVariable})/train${6:$}${3:ResponseVariable}))
	
	mean(abs(testscores-test${6:$}${3:ResponseVariable}))
	mean(abs((testscores-test${6:$}${3:ResponseVariable})/test${6:$}${3:ResponseVariable}))

#Snippet 6: fdr_dm_prunetree (prune the tree based on cp values)
snippet fdr_dm_prunetree
	#Visualize cp values
	plotcp(${1:TreeVariable})  
	
	#Prune tree based on cp value
		pruned.tree<-prune(${1:TreeVariable},cp=0.05)
		printcp(pruned.tree)

## K-Means

snippet fdr_dm_continous
	#creates plots for exploring all continous variables
	library(ggplot2)
	ggplot(melt(${1:dataset}), aes(x=value)) +
	geom_histogram()+
	facet_wrap(~variable)
				
snippet fdr_dm_scal
	#standard scaling of data in data mining
	library(base)
	scale(${1:dataset})
	
	#min-max scaling of data in data mining
	library(BBmisc)
	normalize(${1:dataset})
				
snippet fdr_dm_clusters
	library(factoextra)
	#to find optimal number of clusters
	set.seed(12345)
	#if r throws an error for generating the plot
	while(!is.null(dev.list()))
	dev.off()
	# method : silhouette, wss method
	# k.max : integer value, number of clusters
	fviz_nbclust(${1:scaled_data}, kmeans, method="${2:method}",k.max=${3:num})
				
snippet fdr_dm_K_Means
	library(stats)
	library(factoextra)
	#creating clusters with centers obtains from silhoutte/wss/business context
	#centers and nstart - integer value
	cluster <- kmeans(${1:scaled_data}, centers=${1:num}, nstart=${2:num})
	
	#plotting the clusters
	while(!is.null(dev.list()))
	dev.off() #use if r throws error while displaying the plot
	fviz_cluster(Cluster_wss, data = ${1:scaled_data})
	
								
snippet fdr_dm_agglomerative
	library(stats)
	library(cluster)
	library(dendextend)
	# hierarchical clustering and visualization through agnes
	# method , based on datatype - euclidean, Manhattan, Maximum ,Canberra, Binary, Minkowski
	dist.matrix <- dist(${1:scaled_data}, method="${2:method}")
	
	# method - Average, Single, Ward, Weighted 
	h1.comp.eucl=agnes(dist.matrix,method="${3:method}") 
	
	# plotting the tree
	pltree(h1.comp.eucl, cex = 0.6, hang = -1, main = "Dendrogram of agnes")
	
	#agglomerative coefficient
	h1.comp.eucl$ac 
	
	# use a value of k for stopping the dendogram 
	cut_avg <- cutree(h1.comp.eucl, k=${4:num})

	#visualizing the clusters with colors
	avg_dend_obj <- as.dendrogram(h1.comp.eucl)
	avg_col_dend <- color_branches(avg_dend_obj,k = ${4:num})
	plot(avg_col_dend)


snippet fdr_dm_divisive
	library(stats)
	library(cluster)
	# hierarchical clustering and visualization through Diana
	hc4 <- diana(${1:scaled_data})
	hc4$dc
	
	# plotting the tree
	pltree(hc4, cex = 0.6, hang = -1, main = "Dendrogram of diana")
	clust <- cutree(hc4, k = ${2:num})

	pltree(hc4, hang=-1, cex = 0.6,main ="Dendrogram of diana" )
	rect.hclust(hc4, k = ${2:num}, border = 2:10)
	
## Market Basket Analysis
	
snippet fdr_dm_transactional_dataset
	library(arules)
	library(base)
	library(RColorBrewer)
	
	# id - any unique column in the dataset
	# description - list of products
	trans.data <- as(split(${1:dataset}$description, ${1:dataset}$id), "transactions")
	inspect(trans.data)
	
	# visualizing the top products
	# type="relative","absolute"
	itemFrequencyPlot(trans.data, 
	topN=10, 
	type="absolute",
	col=brewer.pal(8,'Pastel2'), 
	main="Absolute Item Frequency Plot")
	
snippet fdr_dm_mining_rules
	library(arules)
	library(tidyverse)

	#Mining the rules 
	association.rules <- apriori(${1:transactional_data}, parameter = list(supp=${2:num}, 
	conf=${3:num},
	maxlen=${4:num}))

	summary(association.rules)
	
	# visualizing rules in the form of a table
	rules.table <- inspect(association.rules[1:10])
	colnames(rules.table)[2] <- "direction"
	rules.table %>% arrange(desc(lift)) %>% select(lhs,rhs,lift,confidence,count)
	
snippet fdr_dm_product_mining_rule
	library(arules)
	library(tidyverse)
	
	#rule for a particular product
	product.rule  = apriori(${1:transactional_data}, 
	parameter = list(supp=${2:num}, conf=${3:num}),
	appearance = list(default="lhs",rhs="${4:Product}"))

	product.rule.table <- inspect(product.rule)
	colnames(product.rule.table)[2] <- "direction"
	product.rule.table %>% 
	arrange(desc(lift)) %>% 
	select(lhs,rhs,lift,confidence,count)

snippet fdr_dm_interactive_graph
	#Interactive graphs
	#by: confidence, lift , support
	top10Rules <- head(${1:association.rules}, n = ${2:num}, by = "confidence")
	plot(top10Rules, method = "graph",  engine = "htmlwidget")
	
