[["data-mining.html", "Chapter 5 Data Mining 5.1 Supervised 5.2 Unsupervised", " Chapter 5 Data Mining 5.1 Supervised 5.1.1 KNN 5.1.1.1 Choose K snippet fdr_dm_ktune The first step in employing a KNN model is to define the value of k. In this algorithm technique, k is the number of neighbors to assess before determining the value of the current observation. A low k results in a higher variance model (and will lean towards overfitting), whereas a high k results in a higher bias model (and will lean towards underfitting). A common practice is to start with a k = sqrt(n) (where n is the number of samples in the training dataset) and tune this parameter utilizing a validation set or cross-validation. library(caret) library(ggplot2) set.seed(12345) grid = expand.grid(k = seq(1:20)) #Can adjust the range of k if needed tuned_knn = caret::train(factor(ResponseVariable) ~., method= &quot;knn&quot;, data = subset(train, select = -c(Column1ToExclude, Column2ToExclude)), trControl = trainControl(method = &#39;cv&#39;, number = 3, search = &quot;grid&quot;), tuneGrid = grid) tuned_knn$results #Plot results to determine which k maximizes accuracy ggplot(tuned_knn$results, aes(x=k, y=Accuracy)) + geom_line() #The following line of code works as well tuned_knn$results[which.max(tuned_knn$results$Accuracy),] 5.1.1.2 Measuring Distance Euclidean distance is the base measure used in most KNN functions. However, there are other well-known distance measures that can be used such as cosine similarity measure, Minkowski distance, and Chi-square. Alternatively, users can also create their own, more creative distance metrics based on knowledge of the data used. Each distance measurement will affect the results differently. 5.1.1.3 Classification/Prediction Rules The output or value of a new observation is based on the nearest neighbors of the new observation. However, there are many different ways to have those neighbors define the new observation. Classification: majority rules is the most common classification rule, with the most frequent target outcome value within the nearest neighbors being used to determine the classification. If using this method, it is best to set an odd k value such that there are no ties in voting (if there are two classes). Another example for a classification rule is weighting the vote by the nearness of the neighbor. Alternatively, the user can set another rule if it is expected to fit the data better. Prediction: common rules are using the mean or median of the nearest neighbors. Again, the user can set another rule if it is expected to create better predictions. 5.1.1.4 Assumptions and Other Notes snippet fdr_dm_knn KNN is a non-parametric algorithm with no assumptions about the underlying data (other than having a representative training data set). Additionally, it is considered a lazy learner algorithm as it does not learn from the data, but instead stores it and makes classifications/predictions based on the rules set by the user. It is a very simple and flexible model that allows for creativity by the user. However, it can be computationally expensive when classifying new observations, requires storage for the training data, is susceptible to noise, and can require a lot of data preprocessing for distance metrics. Additionally, the model can produce very different results depending on the user-defined parameters and decisions. In order to run KNN in R, all categorical variables must first be converted into numeric variables. #Running KNN requires specifying: ## (1) train.x: Training predictor variable(s) values ## (2) test.x: Test predictor variable(s) values ## (3) train.y: Training response variable values ## (4) k: Number of clusters library(class) predict.test=class::knn(train.x,test.x,train.y,k=NumberOfClusters) #Determine misclassification rate sum(predict.test != train.y)/length(train.y) 5.1.2 CART: Classification and Regression Trees 5.1.2.1 Data Processing Decision trees can handle categorical, continuous, and discrete data. However, ordinal variables tend to be treated as continuous to preserve the importance of order. Decision trees can also handle missing values by setting those values as a missing category. Before building the model, create separate train and test datasets (a validation dataset is optional). The decision tree will be trained on the training dataset and evaluated on the test dataset. 5.1.2.2 Choosing Best Splits Classification: maximizing purity is the determining factor in identifying the best splits within classification trees. Purity measures how homogeneous the nodes are following split points. Most commonly, Gini and Entropy are used as measures of impurity to calculate the best splits. Regression: minimizing SSE (Sum of Squared Errors) determines the splits within the tree. 5.1.2.3 Model Building and Predictions snippet fdr_dm_classtree snippet fdr_dm_regtree After determining the splits and processing the data, the next step is to build the model. There are several functions within R and Python that will help in easily creating a regression or classification tree. When building the model, the training data set is used to find the best splits for predictions. In a classification tree, the prediction is the most common class in the final node. In a regression tree, the prediction is the average value of the target variable in the final node. For both instances, users can change the determination criteria in the final node if desired. #Creates a classification tree library(rpart) library(rpart.plot) library(ggplot2) BC.tree = rpart(ResponseVariable ~ . , data = train, method = &#39;class&#39;, parms = list(split=&#39;gini&#39;)) ## or &#39;information&#39; summary(BC.tree) #Print the actual tree print(BC.tree) rpart.plot(BC.tree) #Variable importance BC.tree$variable.importance varimp.data = data.frame(BC.tree$variable.importance) varimp.data$names = as.character(rownames(varimp.data)) ggplot(data = varimp.data,aes(x = fct_reorder(names,BC.tree$variable.importance), y = BC.tree$variable.importance)) + geom_bar(stat=&quot;identity&quot;) + coord_flip() + labs(x=&quot;Variable Name&quot;,y=&quot;Variable Importance&quot;) library(rpart) library(rpart.plot) library(ggplot2) R.tree = rpart(ResponseVariable ~ . , data = train , method = &#39;anova&#39; #Regression tree , control = rpart.control(minsplit = 10)) #the minimum number of observations that must exist in a node in order for a split to be attempted. summary(R.tree) printcp(R.tree) #Print the actual tree print(R.tree) rpart.plot(R.tree) #Variable importance R.tree$variable.importance varimp.data = data.frame(R.tree$variable.importance) varimp.data$names = as.character(rownames(varimp.data)) ggplot(data = varimp.data,aes(x = fct_reorder(names,R.tree$variable.importance), y = R.tree$variable.importance)) + geom_bar(stat=&quot;identity&quot;) + coord_flip() + labs(x=&quot;Variable Name&quot;,y=&quot;Variable Importance&quot;) 5.1.2.4 Evaluation snippet fdr_dm_evaluation Confusion matrices, accuracy scores, and F1 scores are the most common evaluation metrics for classification problems. MAE, MAPE, RMSE, R-Squared, and Adjusted R-Squared are the most common evaluation metrics for regression analysis. #Classification Evaluation metrics #Use CART model to predict new values for train and test datasets trainscores = predict(ClassificationTreeVariable, type=&#39;class&#39;) #Class for classification trees; vector for regression trees testscores = predict(ClassificationTreeVariable, test, type=&#39;class&#39;) #Training/testing misclassification rate: sum(trainscores!=train$ResponseVariable)/nrow(train) sum(testscores!=test$ResponseVariable)/nrow(test) #Regression Evaluation metrics #Use CART regression model to predict new values for train and test datasets trainscores = predict(RegressionTreeVariable, type=&#39;vector&#39;) #Class for classification trees; vector for regression trees testscores = predict(RegressionTreeVariable, test, type=&#39;vector&#39;) #Training/testing MAE and MAPE (regression): mean(abs(trainscores-train$ResponseVariable)) mean(abs((trainscores-train$ResponseVariable)/train$ResponseVariable)) mean(abs(testscores-test$ResponseVariable)) mean(abs((testscores-test$ResponseVariable)/test$ResponseVariable)) 5.1.2.5 Pruning and Tuning snippet fdr_dm_prunetree Pruning decision trees can be helpful in preventing overfitting and reducing the computational power required to create the model. In a classification tree, pruning occurs by removing the nodes in a bottom-up fashion by first removing the splits with the lowest gain values (while optimizing performance on the validation dataset). For regression trees, pruning can be determined by assessing cross-validation error values and employing the One Standard Error rule. This includes evaluating the split with the lowest cross-validation error value and removing all splits within one standard error of that split. After creating and evaluating a model, it may be advantageous to tune the parameters available. Tuning grids can be used to find the optimal parameters on a training dataset that will help with prediction values for the validation and test sets. Depending on the language and package used, parameters may be different. #Visualize cp values plotcp(TreeVariable) #Prune tree based on cp value pruned.tree&lt;-prune(TreeVariable,cp=0.05) printcp(pruned.tree) 5.2 Unsupervised 5.2.1 Data Clustering Clustering allows groups to be found within a dataset. It is unsupervised, so we do not know which group an observation belongs to in advance. Evaluating cluster results is nontrivial as the target variable is not known. However, the Silhouette Coefficient and Dunns Index are two evaluation options. More information can be found here. Hard clustering means each observation can only belong to one cluster. Fuzzy clustering gives observations probabilities of being in the different clusters rather than assigning them to any one cluster. 5.2.1.1 Hard Hard clustering assigns each observation to only one cluster. Flat and hierarchical are two types of hard clustering. Flat clustering methods require a resolution parameter, while hierarchical clustering methods do not need a resolution parameter (which will be determined automatically). 5.2.1.1.1 Flat Flat clustering is a type of hard clustering, meaning each observation can only belong to one cluster. Flat clustering requires a resolution parameter, such as how many clusters to create (k) or step size (epsilon). 5.2.1.1.1.1 K-means K-means is a type of hard clustering, meaning each observation can only belong to one cluster. It is also flat rather than hierarchical so the number of clusters must be specified. There are a variety of ways to pick the number of clusters (e.g., elbow plot, gap statistic, silhouette method, etc.). K-means requires continuous variables, so all categorical variables must first be dummy coded. Outliers greatly affect the results, so continuous variables should be standardized. The k-means algorithm will find the centroid (mean) of each variable by minimizing the sum of squares within (SSW). SSW is the clustering name for SSE. #standard scaling of data in data mining library(base) scale(dataset) #min-max scaling of data in data mining library(BBmisc) normalize(dataset) # methodologies to find optimal number of clusters - library(factoextra) #to find optimal number of clusters set.seed(12345) #if r throws an error for generating the plot while(!is.null(dev.list())) dev.off() # method : silhouette, wss method # k.max : integer value, number of clusters fviz_nbclust(scaled_data, kmeans, method=&quot;method&quot;,k.max=num) library(stats) library(factoextra) #creating clusters with centers obtains from silhoutte/wss/business context #centers and nstart - integer value cluster &lt;- kmeans(scaled_data, centers=scaled_data, nstart=5) #plotting the clusters while(!is.null(dev.list())) dev.off() #use if r throws error while displaying the plot fviz_cluster(Cluster_wss, data = scaled_data) #summarizing the clusters with respect to the original dataset profile.kmeans &lt;- cbind(data,cluster_wss$cluster) all.k &lt;- profile.kmeans %&gt;% group_by(cluster_wss$cluster) %&gt;% summarise(mean.Channel=mean(Channel), mean.Region=mean(Region), mean.Fresh=mean(Fresh), mean.Milk=mean(Milk), mean.Grocery=mean(Grocery), mean.Frozen=mean(Frozen), mean.Detergents_Paper=mean(Detergents_Paper), mean.Delicassen=mean(Delicassen)) 5.2.1.1.1.2 DBSCAN DBSCAN is a type of hard clustering, meaning each observation can only belong to one cluster. It is also a type of flat clustering because epsilon needs to be specified, which the algorithm will pick automatically. Epsilon is the radius of the circle the algorithm looks within when enlarging the cluster size, and can be thought of as a step size. Unlike k-means, it is a spatial clustering algorithm, which clusters based on a distance matrix. A downside to DBSCAN is that since it is a spatial clustering algorithm, it can confuse semantic meanings when the observations occupy the same space. For this reason, it is a good idea to plot the data first to see if clustering spatially is a good option. 5.2.1.1.2 Hierarchical Hierarchical clustering is a type of hard clustering, meaning each observation can only belong to one cluster. The algorithm chooses the resolution parameter (such as the number of clusters) automatically. Dendrograms are often used to visualize hierarchical clustering output. There are two main types of hierarchical clustering: agglomerative and divisive. 5.2.1.1.2.1 Agglomerative Agglomerative clustering is a type of hard clustering, meaning each observation can only belong to one cluster. It is also a type of hierarchical clustering and so you dont have to specify the number of clusters; the algorithm will pick for you. Agglomerative clustering begins with all observations in their own individual clusters and combines them one at a time based on dissimilarity until they are all in one cluster. There are a number of distance measures to compute the dissimilarity matrix (e.g., Euclidean, Manhattan, etc.). There are also different linkage options (e.g., single, average, maximum, etc.), which should be chosen based on the cluster validation metric being used. Agglomerative clustering requires continuous variables, so all categorical variables must first be dummy coded. Outliers greatly affect the results, so continuous variables should be standardized. Use a dendrogram to visualize. library(stats) library(cluster) library(dendextend) # hierarchical clustering and visualization through agnes # method , based on datatype - euclidean, Manhattan, Maximum ,Canberra, Binary, Minkowski dist.matrix &lt;- dist(scaled_data, method=&quot;method&quot;) # method - Average, Single, Ward, Weighted h1.comp.eucl=agnes(dist.matrix,method=&quot;method&quot;) # plotting the tree pltree(h1.comp.eucl, cex = 0.6, hang = -1, main = &quot;Dendrogram of agnes&quot;) #agglomerative coefficient h1.comp.eucl # use a value of k for stopping the dendogram cut_avg &lt;- cutree(h1.comp.eucl, k=num) #visualizing the clusters with colors avg_dend_obj &lt;- as.dendrogram(h1.comp.eucl) avg_col_dend &lt;- color_branches(avg_dend_obj,k = 5) plot(avg_col_dend) 5.2.1.1.2.2 Divisive Divisive clustering is a type of hard clustering, meaning each observation can only belong to one cluster. It is also a type of hierarchical clustering and so you dont have to specify the number of clusters; the algorithm will pick for you. Divisive clustering begins with all observations in one cluster and separates them one at a time based on dissimilarity until they areall in their own individual clusters. There are a number of distance measures to compute the dissimilarity matrix (e.g., Euclidean, Manhattan, etc.). Use a dendrogram to visualize. library(stats) library(cluster) # hierarchical clustering and visualization through Diana hc4 &lt;- diana(scaled_data) hc4 # plotting the tree pltree(hc4, cex = 0.6, hang = -1, main = &quot;Dendrogram of diana&quot;) clust &lt;- cutree(hc4, k = 5) pltree(hc4, hang=-1, cex = 0.6,main =&quot;Dendrogram of diana&quot; ) rect.hclust(hc4, k = 5, border = 2:10) 5.2.1.2 Fuzzy Fuzzy clustering gives observations probabilities of being in the different clusters rather than assigning them to any one cluster. 5.2.1.2.1 FCM Fuzzy c-means clustering (FCM) is a type of fuzzy clustering. Observations are given probabilities of being in the different clusters rather than being assigned to any one cluster. The number of clusters and iterations need to be specified. It randomly assigns observations to clusters initially and then corrects clusters with each iteration to minimize error. More information on FCM can be found here. 5.2.1.2.2 GMM Gaussian Mixture Model (GMM) clustering is a type of fuzzy clustering. Observations are given probabilities of being in the different clusters rather than being assigned to any one cluster. If using Gaussian rather than frequentist analytic approaches, these probabilities can serve at the posterior distribution. GMMs use maximum likelihood estimation and assume the data is made up of Gaussian distributions. Information from both the centers of the distributions and covariance matrices are used to define the elliptical density groupings into clusters. A downside is that GMM may not converge if there are not enough representatives from each cluster. More information on GMM can be found here. 5.2.2 Assocation Analysis 5.2.2.1 Data Processing To conduct association analysis, transactional data must be provided in a wide format. library(arules) library(base) library(RColorBrewer) # id - any unique column in the dataset # description - list of products trans.data &lt;- as(split(dataset, dataset), &quot;transactions&quot;) inspect(trans.data) # visualizing the top products # type=&quot;relative&quot;,&quot;absolute&quot; itemFrequencyPlot(trans.data, topN=10, type=&quot;absolute&quot;, col=brewer.pal(8,&#39;Pastel2&#39;), main=&quot;Absolute Item Frequency Plot&quot;) 5.2.2.2 Finding Association Rules There are built-in functions in R (arules) that can help identify association rules. It is common practice to establish minimum support and confidence values that must be met for the result to be printed. There are other parameters to create more specific rules, such as setting a minimum lift value. library(arules) library(tidyverse) #Mining the rules association.rules &lt;- apriori(transactional_data, parameter = list(supp=0.001, conf=0.8, maxlen=10)) summary(association.rules) # visualizing rules in the form of a table rules.table &lt;- inspect(association.rules[1:10]) colnames(rules.table)[2] &lt;- &quot;direction&quot; rules.table %&gt;% arrange(desc(lift)) %&gt;% select(lhs,rhs,lift,confidence,count) 5.2.2.3 Assessing Association Rule Strength There are three main statistics in measuring the strength of an association rule: 1) Support: the probability of seeing these items together. 2) Confidence: the probability of seeing the consequent (second item) given the antecedent (first item) 3) Lift: how much more likely the consequent is present given that the antecedent is already present than just seeing the consequent. Lifts higher than 1 are desirable. library(arules) library(tidyverse) #rule for a particular product product.rule = apriori(transactional_data, parameter = list(supp=0.001, conf=0.7), appearance = list(default=&quot;lhs&quot;,rhs=&quot;Product&quot;)) product.rule.table &lt;- inspect(product.rule) colnames(product.rule.table)[2] &lt;- &quot;direction&quot; # visualizing rules in the form of a table product.rule.table %&gt;% arrange(desc(lift)) %&gt;% select(lhs,rhs,lift,confidence,count) #Interactive graphs #by: confidence, lift , support top10Rules &lt;- head(association.rules, n = 10, by = &quot;confidence&quot;) plot(top10Rules, method = &quot;graph&quot;, engine = &quot;htmlwidget&quot;) 5.2.2.4 Miscellaneous Notes There is no time aspect in association analysis. Instead, we are analyzing what products or items are to be bought, removed, etc., simultaneously. When assessing two items, the support and lift will be the same when switching the order of the antecedent and consequent. However, the confidence will be different. "]]
