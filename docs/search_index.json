[["linear-regression.html", "Chapter 2 Linear Regression 2.1 Select Alpha Level 2.2 Create Linear Model of All Variables 2.3 Global F-Test 2.4 Assumptions 2.5 Influential Observations / Outliers 2.6 Variable Selection 2.7 Model Evaluation 2.8 Summarize Findings", " Chapter 2 Linear Regression 2.1 Select Alpha Level There are three ways to select an alpha level prior to modeling. You can stick with the standard 0.05 cutoff, use the below sample size table (from a paper by Raftery), or use the below calculation based on sample size. snippet fdr_alpha #Determines alpha value based on sample size and BIC criterion alpha = 1-pchisq(log(nrow(dataset)),1) 2.2 Create Linear Model of All Variables Assuming you have a reasonable number of variables, start by putting them all into a model to predict the target. Variable selection/ reduction will occur later in the process. If you have a large number of variables, you can consider variable reduction or a different analytical technique. 2.3 Global F-Test Once the linear model is created, a Global F test will discern if any of the variables are significant in predicting y. If no variables are significant, you should re-evaluate the variables or the model selected for this business problem. Assuming at least one variable is significant, continue with the modeling process. snippet fdr_globalF #Global F test: Determines if at least one variable is significant lm.model &lt;- lm(targetvar ~ predvar1 + predvar2, data = dataset) summary(lm.model) 2.4 Assumptions Next, you need to assess a number of assumptions that come with linear regression modeling. First evaluate if the predictor variables have a linear relationship with the response by evaluating their shape when plotted. If the variable looks like a quadratic (curve), polynomial, or not straight consider going back and creating higher order terms. 2.4.1 Linearity Check if they are linear (yes, keep moving) (No) If they look polynomial/ quadratic/ etc. reconsider higher order terms snippet fdr_assumptions_linearity #Check to see if the below plot is random with an average value of zero library(ggplot2) lm.model=lm(targetvar~.,data=dataset) ggplot(lm.model,aes(x=fitted(lm.model),y=resid(lm.model))) + geom_point(color=&quot;blue&quot;)+ labs(x=&quot;Predicted Values&quot;,y=&quot;Residuals&quot;) 2.4.2 Independent Random Errors Next check is the errors are independent by inspecting if the residuals show a pattern. This can also be determined statistically with the Durbin-Watson test. snippet fdr_assumptions_independence #Can use plot to visually see time-dependence or use Durbin-Watson test library(ggplot2) library(lmtest) lm.model=lm(targetvar~.,data=dataset) ggplot(train,aes(x=timevariable,y=lm.model$residuals)) + #i. Visual Plot geom_point(color=&quot;blue&quot;) + labs( x=&quot;Time&quot;,y=&quot;Residuals&quot;) lm.model=lm(targetvar~timevariable,data=dataset) dwtest(lm.model,alternative=&quot;greater&quot;) #ii. Durbin-Watson test 2.4.3 Random Errors Normally Distributed Next, there are three ways to check that the random errors are normally distribution. You can evaluate a histogram of the residuals, create a qq plot and look for a straight line, or do a formal test. The formal test is either Anderson-Darling or Shapiro-Wilk. Shapiro-Wilk is commonly used for smaller data sets. If the errors are not normal, consider a transformation such as box-cox or a log of the xs. snippet fdr_assumptions_normdist #Check to see if the below plot has a constant spread over the range of predicted values #Can use QQ-plot, Anderson-Darling test, or Shapiro-Wilk test library(ggplot2) library(nortest) lm.model=lm(targetvar~.,data=dataset) ggplot(data = dataset, aes(sample = lm.model$residuals)) + #i. QQ-Plot stat_qq() + stat_qq_line() ad.test(lm.model$residuals) #ii. Anderson-Darling test. Gives more weight to tails shapiro.test(lm.model$residuals) #iii. Shapiro-Wilk test. Better for smaller data sets 2.4.4 Random Error Constant Variance Then you need to further inspect the residuals by looking for patterns. If the residuals visually have a constant variance, then the random error assumption is met. This can also be evaluated statistically with the Spearman Rank Correlation. Values close to 0 mean the variance is constant. If its not constant (visually or a spearman rank correlation close to 1 or -1) consider alternative options such as weighted least squares, transforming the data, or a different distribution (ex: Poisson) snippet fdr_assumptions_constvar #Check to see if the below plot has a constant spread over the range of predicted values library(ggplot2) lm.model=lm(targetvar~.,data=dataset) ggplot(lm.model,aes(x=fitted(lm.model),y=resid(lm.model))) + geom_point(color=&quot;blue&quot;) + labs(x=&quot;Predicted Values&quot;,y=&quot;Residuals&quot;) 2.4.5 Single Predictor? The final assumption to check is multicollinearity. If you only have 1 predictor, you can skip this step. 2.4.6 Multiple Predictors 2.4.6.1 Check for Multicollinearity If you have multiple predictors check multicollinearity with VIF. If VIF is greater than 10, there is multicollinearity and you should remove the related variable or transform the variable. If the VIF is less than 10, continue on to influential observations/ outliers. snippet fdr_assumptions_multicol #Check for correlation among predictor variables or calculate the variance inflation factor (VIF) library(car) #First filter dataframe to only contain numeric variables dataset_num = dataset[,c(5,6,82)] cor(dataset_num) #i. Correlation among predictor variables lm.model=lm(targetvar~.,data=dataset) vif(lm.model) #ii. Variance inflation factor (VIF) 2.5 Influential Observations / Outliers Once all of the assumptions are met, consider handling influential observations and outliers. The strategies below to help identify influential observations, and 5 strategies to handle them once detected. 2.5.1 Identify influential observations Internally Studentized residuals (good for detecting outliers) Externally Studentized residuals (good for detecting outliers) Cooks D (good for detecting influential observations) DFFITS (good for detecting influential observations) DFBETAS (good for detecting influential observations) Hat values (good for detecting influential observations) snippet fdr_influential #Tests for finding influential observations library(ggplot2) library(pdp) lm.model=lm(targetvar~.,data=dataset) n.index = seq(1, nrow(dataset)) p = length(lm.model$coefficients) #i. Cook&#39;s D D.cut= 4/(nrow(dataset)-p-1) c = ggplot(lm.model,aes(x=n.index,y=cooks.distance(lm.model))) + geom_point(color=&quot;orange&quot;) + geom_line(y=D.cut) + labs(title = &quot;Cook&#39;s D&quot;,x = &quot;Observation&quot;,y =&quot;Cook&#39;s Distance&quot;) #ii. DFFITS df.cut=2*(sqrt((p)/nrow(dataset))) d = ggplot(lm.model,aes(x=n.index,y=dffits(lm.model))) + geom_point(color=&quot;orange&quot;) + geom_line(y=df.cut) + geom_line(y=-df.cut) + labs(title = &quot;DFFITS&quot;,x=&quot;Observation&quot;,y=&quot;DFFITS&quot;) #Observations that are outside of dffits dataset[dffits(lm.model) &gt; df.cut | dffits(lm.model) &lt; -df.cut,] #iii. DFBETAS (copy and paste for each variable) db.cut=2/sqrt(nrow(dataset)) e = ggplot(lm.model,aes(x=n.index,y=dfbetas(lm.model)[,&#39;variablename&#39;])) + geom_point(color=&quot;orange&quot;) + geom_line(y=db.cut) + geom_line(y=-db.cut) + labs(title = &quot;DFBETA for variablename&quot;,x=&quot;Observation&quot;,y=&quot;DFBETAS&quot;) #iv. Hhat hat.cut=2*(p+1)/nrow(dataset) f = ggplot(lm.model,aes(x=n.index,y=hatvalues(lm.model))) + geom_point(color=&quot;orange&quot;) + geom_line(y=hat.cut) + labs(title = &quot;Hat values&quot;,x=&quot;Observation&quot;,y=&quot;Hat Values&quot;) #Print all plots in a grid. Add additional variables for DFBETAS if desired grid.arrange(c,d,e,f) 2.5.2 Handle Outliers Recheck the data to ensure that no transcription or data entry errors occurred. If the data is valid, one possible explanation is that the model is not adequate. A model with higher-order terms, such as polynomials and interactions between the variables, might be necessary to fit the data well. Nonlinear model Determine the robustness of the inference bv running the analysis both with and without the influential observations. Robust Regression (Covered Later in Program) Weighted Least Squares (WCS) snippet fdr_outliers #Tests for finding outliers library(ggplot2) library(pdp) lm.model=lm(targetvar~.,data=dataset) n.index = seq(1, nrow(dataset)) #i. Internal Studentized Residuals a = ggplot(lm.model,aes(x=n.index,y=rstandard(lm.model))) + geom_point(color=&quot;orange&quot;) + geom_line(y=-3) + geom_line(y=3) + labs(title = &quot;Internal Studentized Residuals&quot;,x=&quot;Observation&quot;,y=&quot;Residuals&quot;) #ii. External Studentized Residuals b = ggplot(lm.model,aes(x=n.index,y=rstudent(lm.model)))+ geom_point(color=&quot;orange&quot;) + geom_line(y=-3) + geom_line(y=3) + labs(title = &quot;External Studentized Residuals&quot;,x=&quot;Observation&quot;,y=&quot;Residuals&quot;) grid.arrange(a,b) 2.6 Variable Selection When the data is postured and ready for analysis, start with variable selection. At this point you can choose between many selection techniques such as forward, backward, stepwise, and regularization (L1, L2, or elastic net). If you have a lot of variable youre looking to cut back on, consider backward selection or variable clustering techniques. You also need to choose a model evaluation statistic of AIC, BIC, or p-value. snippet fdr_varselect_linreg #Create training model library(dplyr) library(glmnet) set.seed(19054) dataframe &lt;- dataframe %&gt;% mutate(id = row_number()) #Only necessary if data does not have id-like column train = dataframe %&gt;% sample_frac(0.7) #70% of data points are set aside for training (can be changed) test &lt;- anti_join(dataframe, train, by = &#39;id&#39;) #The remainder are assigned for testing train = train[,c(colnumstovalidate)] #Select columns you want to analyze #train = train[,!(colnames(train) %in% &quot;id&quot;)] #Remove id column if not used in modeling # Create full model and empty model full.model = lm(yvar ~ . , data = train) empty.model = lm(yvar ~ 1, data = train) # Information Criteria (InfCrit) #AIC: k = 2 #BIC: k = log(nrow(train)) #p-value: k = qchisq(alpha.f , 1, lower.tail = FALSE) #Create model #alpha.f=0.20 #Only needed for p-value calculation for.model = step(empty.model, scope = list(lower = empty.model, upper = full.model), direction = &quot;forward&quot;, k = InfCrit) back.model = step(full.model, scope = list(lower = empty.model, upper = full.model), direction = &quot;backward&quot;, k = InfCrit) step.model = step(empty.model, scope = list(lower = empty.model, upper = full.model), direction = &quot;both&quot;, k = InfCrit) #Regularized regression for variable selection (LASSO example) train_x &lt;- model.matrix(yvar ~ ., data = train)[, -1] train_y &lt;- train$&#39;yvar&#39; train_lasso = glmnet(x=train_x, y = yvar, alpha = 1) plot(train_lasso, xvar = &quot;lambda&quot;) 2.7 Model Evaluation After the variables are selected, its time to run the linear regression. Evaluation metrics such as MAE, MAPE, RMSE, and MSE can be computed with the model output. Additionally, the chosen evaluation statistic (AIC, BIC, or adjusted r^2) and be computed on the validation set for comparison and on the test set (or the test rolled into the rest of the data) to report the final metrics. snippet fdr_modeval #Evaluate various metrics for model accuracy library(dplyr) testdataset$pred_lm &lt;- predict(finalmodel, newdata = testdataset) testdataset %&gt;% mutate(lm_APE = 100*abs((yvar - pred_lm)/yvar)) %&gt;% mutate(lm_AE = abs(yvar - pred_lm)) %&gt;% mutate(lm_SE = (yvar - pred_lm)^2) %&gt;% dplyr::summarise(MAPE_lm = mean(lm_APE) , MAE_lm = mean(lm_AE) , MSE_lm = mean(lm_SE) , RMSE_lm = sqrt(mean(lm_SE)) , AIC_lm = AIC(finalmodel) , BIC_lm = BIC(finalmodel) , Ra2_lm = summary(finalmodel)$adj.r.squared) 2.8 Summarize Findings With these final metrics, you can summarize any business insights, findings, and takeaways based on the influential variables, model metrics, and business context. "]]
