---
title: "Linear Regression"
author: "Sam Jasper and Jackson de Oliveira"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  bookdown::gitbook:
      
---
#This is a change!!!

```{r setup, echo=FALSE, warning=FALSE, include=FALSE}
library(DiagrammeR)
# Add a common class name for every chunks
knitr::opts_chunk$set(
  echo = TRUE)

# RTFButton = grViz("digraph {0 [label = 'START HERE!', color='chocolate1', style=filled, URL='#select-alpha-level', fontcolor='blue']}")

```

# Linear Regression

```{r diagram, echo=FALSE, fig.height=14, fig.width=8}

grViz("digraph {
  graph [layout = dot, rankdir = TB]
  
  node [shape = rectangle, fontsize=40, style=filled, color='lightblue1']
  0 [label = 'START HERE!', color='chocolate1', style=filled, URL='#select-alpha-level', fontcolor='blue']
  1 [label = 'Build a linear model with all variables', URL='#create-linear-model-of-all-variables', fontcolor='blue']
  H [label = 'Re-evaluate Variables 
  and/or Model Type']
  10 [label = '1. WLS/IRLS
  2. Transform
  3. Different Distribution']
  13 [label = 'Remove Var(s) or 
  Create new transformed vars']
  14 [label = 'Handle Influential 
  Observations', URL='#influential-observations-outliers', fontcolor='blue']
  15 [label = 'Select Variable Evaluation
  Statistic']
  16 [label = 'Select Variable 
  Selection Technique', URL='#variable-selection', fontcolor='blue']
  17 [label = 'Select Model 
  Evaluation Statistic', URL='#model-evaluation', fontcolor='blue']
  18 [label = 'Summarize
  Findings', URL='#summarize-findings', fontcolor='blue']
  
  node [shape = diamond, fontsize=40, style=filled, color=cornsilk2]
  2 [label = 'Is a global F-Test 
  significant?', URL='#global-f-test', fontcolor='blue']
  3 [label = 'Test Assumptions: 
  Linearity of the mean', URL='#linearity', fontcolor='blue']
  6 [label = 'Residuals Polynomial?
  Use Higher Order']
  4 [label = 'Test Assumptions: 
  Independent Random Errors', URL='#independent-random-errors', fontcolor='blue']
  5 [label = 'Test Assumptions:
  Normality', URL='#random-errors-normally-distributed', fontcolor='blue']
  7 [label = 'Box Cox 
  Transform']
  8 [label = 'Transform 
  X-Values']
  9 [label = 'Test Assumptions: 
  Constant Variance', URL='#random-error-constant-variance', fontcolor='blue']
  11 [label = 'Multiple 
  X-Vars?', URL='#single-predictor', fontcolor='blue']
  12 [label = 'Any VIF 
  > 10?', URL='#check-for-multicollinearity', fontcolor='blue']

  
  # edge definitions with the node IDs
  0 -> 1 [style=bold]
  1 -> 2 [style=bold]
  H -> 1 [style=bold]
  2 -> H [label = 'No', fontsize=40, style=dashed]
  2 -> 3 [label = 'Yes', fontsize=40, style=bold]
  3 -> 4 [label = 'Yes', fontsize=40, style=bold]
  3 -> 6 [label = 'No', fontsize=40, style=dashed]
  6 -> H [label = 'Yes', fontsize=40, style=bold]
  6 -> H [label = 'No', fontsize=40, style=dashed]
  4 -> 5 [label = 'Yes', fontsize=40, style=bold]
  4 -> H [label = 'No', fontsize=40, style=dashed]
  5 -> 7 [label = 'No', fontsize=40, style=dashed]
  7 -> 8 [label = 'No', fontsize=40, style=dashed]
  8 -> H [label = 'No', fontsize=40, style=dashed]
  5 -> 9 [label = 'Normal', fontsize=40, style=bold]
  7 -> 9 [label = 'Normal', fontsize=40, style=bold]
  8 -> 9 [label = 'Normal', fontsize=40, style=bold]
  9 -> 10 [label = 'No', fontsize=40, style=dashed]
  10 -> H [label = 'No', fontsize=40, style=dashed]
  9 -> 11 [label = 'Yes', fontsize=40, style=bold]
  11 -> 12 [label = 'Yes', fontsize=40, style=bold]
  12 -> 13 [label = 'Yes', fontsize=40, style=bold]
  11 -> 14 [label = 'No', fontsize=40, style=dashed]
  13 -> 14 [style=bold]
  12 -> 14 [label = 'No', fontsize=40, style=dashed]
  14 -> 15 [style=bold]
  15 -> 16 [style=bold]
  16 -> 17 [style=bold]
  17 -> 18 [style=bold]
  

  }")

```

## Select Alpha Level
<span style="color:blue;"> *snippet fdr_alpha *</span> 

There are three ways to select an alpha level prior to modeling. You can stick with the standard 0.05 cutoff, use the below sample size table (from a paper by Raftery), or use the below calculation based on sample size.

```{r fdr_alpha_codeblock, eval=FALSE}
#Determines alpha value based on sample size and BIC criterion
alpha = 1-pchisq(log(nrow(dataset)),1)
```

## Create Linear Model of All Variables


Assuming you have a reasonable number of variables, start by putting them all into a model to predict the target. Variable selection/ reduction will occur later in the process. If you have a large number of variables, you can consider variable reduction or a different analytical technique.

## Global F-Test

<span style="color:blue;"> *snippet fdr_globalF* </span> 

Once the linear model is created, a Global F test will discern if any of the variables are significant in predicting y. If no variables are significant, you should re-evaluate the variables or the model selected for this business problem. Assuming at least one variable is significant, continue with the modeling process.

```{r fdr_globalF_codeblock, eval=FALSE}
#Global F test: Determines if at least one variable is significant
lm.model <- lm(targetvar ~ predvar1 + predvar2, data = dataset)
summary(lm.model)
```

## Assumptions

Next, you need to assess a number of assumptions that come with linear regression modeling. First evaluate if the predictor variables have a linear relationship with the response by evaluating their shape when plotted. If the variable looks like a quadratic (curve), polynomial, or not straight consider going back and creating higher order terms. 

### Linearity

<span style="color:blue;"> *snippet fdr_assumptions_linearity* </span> 

Check if they are linear (yes, keep moving)
(No) If they look polynomial/ quadratic/ etc. reconsider higher order terms

```{r fdr_assumptions_linearity_codeblock, eval=FALSE}
#Check to see if the below plot is random with an average value of zero
library(ggplot2)
lm.model=lm(targetvar~.,data=dataset)
ggplot(lm.model,aes(x=fitted(lm.model),y=resid(lm.model))) +
  geom_point(color="blue")+
  labs(x="Predicted Values",y="Residuals")
```


### Independent Random Errors

<span style="color:blue;"> *snippet fdr_assumptions_independence *</span> 

Next check is the errors are independent by inspecting if the residuals show a pattern. This can also be determined statistically with the Durbin-Watson test. 

```{r fdr_assumptions_independence_codeblock, eval=FALSE}
#Can use plot to visually see time-dependence or use Durbin-Watson test
library(ggplot2)
library(lmtest)
lm.model=lm(targetvar~.,data=dataset)

ggplot(train,aes(x=timevariable,y=lm.model$residuals)) +  #i. Visual Plot
  geom_point(color="blue") +
  labs( x="Time",y="Residuals")

lm.model=lm(targetvar~timevariable,data=dataset)
dwtest(lm.model,alternative="greater") #ii. Durbin-Watson test
```

### Random Errors Normally Distributed

<span style="color:blue;"> *snippet fdr_assumptions_normdist *</span> 

Next, there are three ways to check that the random errors are normally distribution. You can evaluate a histogram of the residuals, create a qq plot and look for a straight line, or do a formal test. The formal test is either Anderson-Darling or Shapiro-Wilk. Shapiro-Wilk is commonly used for smaller data sets. If the errors are not normal, consider a transformation such as box-cox or a log of the x's. 

```{r fdr_assumptions_normdist_codeblock, eval=FALSE}
#Check to see if the below plot has a constant spread over the range of predicted values
#Can use QQ-plot, Anderson-Darling test, or Shapiro-Wilk test
library(ggplot2)
library(nortest)
lm.model=lm(targetvar~.,data=dataset)

ggplot(data = dataset, aes(sample = lm.model$residuals)) + #i. QQ-Plot
  stat_qq() +
  stat_qq_line()
  
ad.test(lm.model$residuals) #ii. Anderson-Darling test. Gives more weight to tails
shapiro.test(lm.model$residuals) #iii. Shapiro-Wilk test. Better for smaller data sets
```


### Random Error Constant Variance

<span style="color:blue;"> *snippet fdr_assumptions_constvar *</span> 

Then you need to further inspect the residuals by looking for patterns. If the residuals visually have a constant variance, then the random error assumption is met. This can also be evaluated statistically with the Spearman Rank Correlation. Values close to 0 mean the variance is constant. If it's not constant (visually or a spearman rank correlation close to 1 or -1) consider alternative options such as weighted least squares, transforming the data, or a different distribution (ex: Poisson)

```{r fdr_assumptions_constvar_codeblock, eval=FALSE}
#Check to see if the below plot has a constant spread over the range of predicted values
library(ggplot2)
lm.model=lm(targetvar~.,data=dataset)
ggplot(lm.model,aes(x=fitted(lm.model),y=resid(lm.model))) +
  geom_point(color="blue") +
  labs(x="Predicted Values",y="Residuals")
```


### Single Predictor?

The final assumption to check is multicollinearity. If you only have 1 predictor, you can skip this step. 


### Multiple Predictors
#### Check for Multicollinearity

<span style="color:blue;"> *snippet fdr_assumptions_multicol *</span> 

If you have multiple predictors check multicollinearity with VIF. If VIF is greater than 10, there is multicollinearity and you should remove the related variable or transform the variable. If the VIF is less than 10, continue on to influential observations/ outliers.

```{r fdr_assumptions_multicol_codeblock, eval=FALSE}
#Check for correlation among predictor variables or calculate the variance inflation factor (VIF)
library(car)

#First filter dataframe to only contain numeric variables
dataset_num = dataset[,c(5,6,82)]
cor(dataset_num) #i. Correlation among predictor variables

lm.model=lm(targetvar~.,data=dataset)
vif(lm.model) #ii. Variance inflation factor (VIF)
```

## Influential Observations / Outliers

Once all of the assumptions are met, consider handling influential observations and outliers. The strategies below to help identify influential observations, and 5 strategies to handle them once detected. 

### Identify influential observations 

<span style="color:blue;"> *snippet fdr_influential*</span> 

* Internally Studentized residuals (good for detecting outliers) 
* Externally Studentized residuals (good for detecting outliers)
* Cook's D (good for detecting influential observations)
* DFFITS (good for detecting influential observations)
* DFBETAS (good for detecting influential observations) 
* Hat values (good for detecting influential observations)

```{r fdr_influential_codeblock, eval=FALSE}
#Tests for finding influential observations
library(ggplot2)
library(pdp)
lm.model=lm(targetvar~.,data=dataset)
n.index = seq(1, nrow(dataset))
p = length(lm.model$coefficients)

#i. Cook's D
D.cut= 4/(nrow(dataset)-p-1)

c = ggplot(lm.model,aes(x=n.index,y=cooks.distance(lm.model))) +
      geom_point(color="orange") +
      geom_line(y=D.cut) +
      labs(title = "Cook's D",x = "Observation",y ="Cook's Distance")

#ii. DFFITS
df.cut=2*(sqrt((p)/nrow(dataset)))

d = ggplot(lm.model,aes(x=n.index,y=dffits(lm.model))) +
      geom_point(color="orange") +
      geom_line(y=df.cut) +
      geom_line(y=-df.cut) +
      labs(title = "DFFITS",x="Observation",y="DFFITS")

#Observations that are outside of dffits
dataset[dffits(lm.model) > df.cut | dffits(lm.model) < -df.cut,]

#iii. DFBETAS (copy and paste for each variable)
db.cut=2/sqrt(nrow(dataset))

e = ggplot(lm.model,aes(x=n.index,y=dfbetas(lm.model)[,'variablename'])) + 
      geom_point(color="orange") + 
      geom_line(y=db.cut) + 
      geom_line(y=-db.cut) + 
      labs(title = "DFBETA for variablename",x="Observation",y="DFBETAS")

#iv. Hhat
hat.cut=2*(p+1)/nrow(dataset)

f = ggplot(lm.model,aes(x=n.index,y=hatvalues(lm.model))) +
      geom_point(color="orange") +
      geom_line(y=hat.cut) +
      labs(title = "Hat values",x="Observation",y="Hat Values")

#Print all plots in a grid. Add additional variables for DFBETAS if desired
grid.arrange(c,d,e,f)
```
  
### Handle Outliers

<span style="color:blue;"> *snippet fdr_outliers*</span> 

1. Recheck the data to ensure that no transcription or data entry errors 
  occurred. 
2. If the data is valid, one possible explanation is that 
  the model is not adequate. 
    + A model with higher-order terms, such as polynomials and interactions between the variables, might be necessary to fit the data well. 
    + Nonlinear model 
3. Determine the robustness of the inference bv running the analysis both with and without the influential observations. 
4. Robust Regression (Covered Later in Program) 
5. Weighted Least Squares (WCS)

```{r fdr_outliers_codeblock, eval=FALSE}
#Tests for finding outliers
library(ggplot2)
library(pdp)
lm.model=lm(targetvar~.,data=dataset)
n.index = seq(1, nrow(dataset))

#i. Internal Studentized Residuals
a = ggplot(lm.model,aes(x=n.index,y=rstandard(lm.model))) + 
  geom_point(color="orange") + 
  geom_line(y=-3) + 
  geom_line(y=3) + 
  labs(title = "Internal Studentized Residuals",x="Observation",y="Residuals")

#ii. External Studentized Residuals
b = ggplot(lm.model,aes(x=n.index,y=rstudent(lm.model)))+ 
  geom_point(color="orange") + 
  geom_line(y=-3) + 
  geom_line(y=3) + 
  labs(title = "External Studentized Residuals",x="Observation",y="Residuals")

grid.arrange(a,b)
```

## Variable Selection

<span style="color:blue;"> *snippet fdr_varselect_linreg*</span> 

When the data is postured and ready for analysis, start with variable selection. At this point you can choose between many selection techniques such as forward, backward, stepwise, and regularization (L1, L2, or elastic net). If you have a lot of variable you're looking to cut back on, consider backward selection or variable clustering techniques. You also need to choose a model evaluation statistic of AIC, BIC, or p-value.

```{r fdr_varselect_linreg_codeblock, eval=FALSE}
#Create training model
library(dplyr)
library(glmnet)
set.seed(19054)

dataframe <- dataframe %>% mutate(id = row_number()) #Only necessary if data does not have id-like column
train = dataframe %>% sample_frac(0.7) #70% of data points are set aside for training (can be changed)
test <- anti_join(dataframe, train, by = 'id') #The remainder are assigned for testing
train = train[,c(colnumstovalidate)] #Select columns you want to analyze
#train = train[,!(colnames(train) %in% "id")] #Remove id column if not used in modeling

# Create full model and empty model
full.model = lm(yvar ~ . , data = train)
empty.model = lm(yvar ~ 1, data = train)

# Information Criteria (InfCrit)
#AIC: k = 2
#BIC: k = log(nrow(train))
#p-value: k = qchisq(alpha.f , 1, lower.tail = FALSE)

#Create model
#alpha.f=0.20 #Only needed for p-value calculation

for.model = step(empty.model, scope = list(lower = empty.model, upper = full.model), direction = "forward", k = InfCrit)
back.model = step(full.model, scope = list(lower = empty.model, upper = full.model), direction = "backward", k = InfCrit)
step.model = step(empty.model, scope = list(lower = empty.model, upper = full.model), direction = "both", k = InfCrit)

#Regularized regression for variable selection (LASSO example)
train_x <- model.matrix(yvar ~ ., data = train)[, -1]
train_y <- train$'yvar'

train_lasso = glmnet(x=train_x, y = yvar, alpha = 1)
plot(train_lasso, xvar = "lambda")
```

## Model Evaluation

<span style="color:blue;"> *snippet fdr_modeval*</span> 

After the variables are selected, it's time to run the linear regression. Evaluation metrics such as MAE, MAPE, RMSE, and MSE can be computed with the model output. Additionally, the chosen evaluation statistic (AIC, BIC, or adjusted r^2) and be computed on the validation set for comparison and on the test set (or the test rolled into the rest of the data) to report the final metrics. 

```{r fdr_modeval_codeblock, eval=FALSE}
#Evaluate various metrics for model accuracy
library(dplyr)

testdataset$pred_lm <- predict(finalmodel, newdata = testdataset)

testdataset %>% 
  mutate(lm_APE = 100*abs((yvar - pred_lm)/yvar)) %>% 
  mutate(lm_AE = abs(yvar - pred_lm)) %>% 
  mutate(lm_SE = (yvar - pred_lm)^2) %>% 
  dplyr::summarise(MAPE_lm = mean(lm_APE)
                            , MAE_lm = mean(lm_AE)
                            , MSE_lm = mean(lm_SE)
                            , RMSE_lm = sqrt(mean(lm_SE))
                            , AIC_lm = AIC(finalmodel)
                            , BIC_lm = BIC(finalmodel)
                            , Ra2_lm = summary(finalmodel)$adj.r.squared)
```

## Summarize Findings

With these final metrics, you can summarize any business insights, findings, and takeaways based on the influential variables, model metrics, and business context. 




